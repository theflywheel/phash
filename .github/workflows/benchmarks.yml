name: Performance Benchmarks

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
  schedule:
    - cron: "0 0 * * 0" # Weekly run on Sunday

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # Fetch all history for all branches and tags

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: "1.20"
          check-latest: true

      - name: Install dependencies
        run: go get -v ./...

      # Run benchmarks using the provided script
      - name: Run benchmarks
        run: |
          mkdir -p benchmark_history
          ./run_benchmarks.sh
          # latest.json is now created

      # For PRs, compare with main branch
      - name: Compare PR benchmarks with main
        if: github.event_name == 'pull_request'
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"

          echo "Fetching main branch for comparison..."
          git fetch origin main:main

          echo "PR is comparing branch ${{ github.head_ref }} to main"

          # Save the current PR results
          cp benchmark_history/latest.json pr_latest.json

          # Get the baseline from main branch
          git checkout main -- benchmark_history/latest.json

          if [ ! -f "benchmark_history/latest.json" ]; then
            echo "No baseline found on main branch. Using current results as baseline."
            cp pr_latest.json benchmark_history/latest.json
          else
            # Rename main branch latest.json to main_latest.json
            cp benchmark_history/latest.json main_latest.json
            # Restore PR latest.json
            cp pr_latest.json benchmark_history/latest.json
          fi

          # Compare PR results with main
          ./bench/tools/compare_benchmarks.sh benchmark_history/latest.json main_latest.json

      # For pushes to main, always update the benchmark baseline
      - name: Update benchmark baseline
        if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"

          # Make sure benchmark_history directory exists
          mkdir -p benchmark_history

          # Make sure we have a latest.json file (should be created by run_benchmarks.sh)
          if [ ! -f "benchmark_history/latest.json" ]; then
            echo "Error: benchmark_history/latest.json not found. Check run_benchmarks.sh"
            exit 1
          fi

          # Add and commit the new baseline
          git add benchmark_history/latest.json

          # Create a timestamped copy for historical tracking
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          cp benchmark_history/latest.json benchmark_history/baseline-${TIMESTAMP}.json
          git add benchmark_history/baseline-${TIMESTAMP}.json

          git commit -m "Update benchmark baseline [skip ci]"

          # Push using the GitHub token for authentication
          git push https://${{ github.actor }}:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git HEAD:${GITHUB_REF#refs/heads/}

          echo "Benchmark baseline updated in the repository."

      # Validate benchmark results for PRs and fail if performance degraded
      - name: Validate benchmark improvements
        if: github.event_name == 'pull_request'
        run: |
          # Check if benchmarks have improved or remained stable
          ./bench/tools/validate_benchmarks.sh || (echo "::error::Benchmark performance has degraded! See benchmark results for details." && exit 1)

      # Comment on PR with benchmark results
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            try {
              const benchmarkResults = fs.readFileSync('./benchmark-comparison.json', 'utf8');
              const results = JSON.parse(benchmarkResults);
              
              // Generate a formatted comment with benchmark comparison
              let comment = '## Benchmark Results\n\n';
              comment += '| Benchmark | Before | After | Change |\n';
              comment += '|-----------|--------|-------|--------|\n';
              
              // Add each benchmark to the table
              for (const benchmark of results.benchmarks) {
                const beforeValue = benchmark.baseline_ns_per_op.toFixed(2);
                const afterValue = benchmark.current_ns_per_op.toFixed(2);
                const pctChange = benchmark.percent_change.toFixed(2);
                const changeIcon = pctChange <= 0 ? '✅' : '❌';
                
                comment += `| ${benchmark.name} | ${beforeValue}ns | ${afterValue}ns | ${changeIcon} ${pctChange}% |\n`;
              }
              
              comment += '\n### Summary\n';
              comment += results.improved ? 
                '✅ **Benchmarks have improved or remained stable!**\n' : 
                '❌ **Some benchmarks have degraded in performance.**\n';
              
              // Post comment on PR
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.error('Error processing benchmark results:', error);
              core.setFailed('Failed to process benchmark results');
            }

      # Archive benchmark results
      - name: Archive benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark_history/
            benchmark-comparison.json
            *_benchmark_results.txt
          retention-days: 90
